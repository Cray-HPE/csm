Installation Instructions
=========================

Copyright 2020 Hewlett Packard Enterprise Development LP

NOTE: Docs are also available on the LiveCD at /usr/share/doc/metal/.

Refer to docs/002-LIVECD-SETUP.md and docs/003-LIVECD-STARTUP.md to get a
node up and running using the included LiveCD.

See docs/004-LIVECD-INSTALL-AND-CONFIG.md and docs/005-NCN-DEPLOY.md for
details on installing and configuring a CSM cluster from the LiveCD.

Once the CSM Kubernetes cluster is successfully up and running, run the
commands below to finish installing CSM applications and services. Set
CSM_RELEASE to the CSM release name (e.g. csm-x.y.z). Set SYSTEM_NAME to the
name of the system. Make sure SYSCONFDIR is where the CSI generated config
files can be found.

    pit:~ # export CSM_RELEASE=csm-x.y.z
    pit:~ # export SYSTEM_NAME=eniac
    pit:~ # export SYSCONFDIR="/var/www/ephemeral/prep/${SYSTEM_NAME}"

Start the install:

    pit:~ # cd /var/www/ephemeral/$CSM_RELEASE
    pit:/var/www/ephemeral/$CSM_RELEASE # ./install.sh

Note that install.sh will exit with instructions that may be copied and pasted
to switch DNS settings from dnsmasq to Unbound and then to continue the
installation. For example:

    pit:/var/www/ephemeral/csm-0.7.24 # ./install.sh
    ...

    Continue with the installation after performing the following steps to switch
    DNS settings from dnsmasq on the pit server to Unbound running in Kubernetes:

    1. Unbound is listening on 10.92.100.225, verify it is working by resolving
       e.g., ncn-w001.nmn:

        pit:/var/www/ephemeral/csm-0.7.24 # dig "@10.92.100.225" +short ncn-w001.nmn

    2. Run the following two commands on all NCN manager, worker, and storage
       nodes as well as the pit server:

        # sed -e "s/^\(NETCONFIG_DNS_STATIC_SERVERS\)=.*$/\1=\"10.92.100.225"/" -i /etc/sysconfig/network/config
        # netconfig update -f

    3. Stop dnsmasq on the pit server:

        pit:/var/www/ephemeral/csm-0.7.24 # systemctl stop dnsmasq
        pit:/var/www/ephemeral/csm-0.7.24 # systemctl disable dnsmasq

    4. Continue with the installation:

        pit:/var/www/ephemeral/csm-0.7.24 # ./install.sh --continue


Mitigating Install Failures
---------------------------

The install.sh script changes cluster state and should not simply be rerun in
the event of a failure without careful consideration of the specific error. It
may be possible to resume installation from the last successful command
executed by install.sh, but admins will need to appropriately modify install.sh
to pick up where the previous run left off. (Note: The install.sh script runs
with `set -x`, so each command will be printed to stderr prefixed with the
expanded value of PS4, e.g., `+ `.)

Known potential issues with suggested fixes, listed in the order they may
occur:

*   `error: not ready: https://packages.local`

    In general, this error indicates that from the callerâ€™s perspective, Nexus
    is not ready to receive writes. More accurately, it most likely means that
    a Nexus setup utility was unable to connect to Nexus via the
    "packages.local" name. Since the install does not attempt to connect to
    packages.local until Nexus has been successfully deployed, the error does
    not usually indicate something is actually wrong with Nexus. Instead, it is
    most commonly a network issue with e.g., name resolution (i.e., DNS), IP
    routes from the pit node, switch misconfiguration, or Istio ingress.

    Verify that packages.local resolves to **ONLY** the load balancer IP for
    the istio-ingressgateway service in the istio-system namespace, typically
    10.92.100.71. If name resolution returns addresses on other networks (e.g.,
    HMN) this must be corrected. Prior to DNS/DHCP hand-off to Unbound, these
    settings are controlled by dnsmasq. Unbound settings are based on SLS
    settings in sls_input_file.json and must be updated via the Unbound
    manager.

    If packages.local resolves to the correct addresses, verify basic
    connectivity using ping. If `ping packages.local` is unsuccessful, verify
    the IP routes from the pit node to the NMN load balancer network, e.g., the
    typical `ip route` configuration is `10.92.100.0/24 via 10.252.0.1 dev
    vlan002`. If pings are successful, try checking the status of Nexus by
    running `curl -sS https://packages.local/service/rest/v1/status/writable`.
    If the connection times out, it indicates there is a more complex
    connection issue. Verify switches are configured properly and BGP peering
    is operating correctly, see docs/400-SWITCH-BGP-NEIGHBORS.md for more
    information. Lastly, check Istio and OPA logs to see if connections to
    packages.local are not reaching Nexus, e.g., perhaps due to an
    authorization issue.

    If https://packages.local/service/rest/v1/status/writable returns an HTTP
    code other than 200 OK, it indicates there is an issue with Nexus. Verify
    that the `loftsman ship` deployment of the nexus.yaml manifest was
    successful. If `helm status -n nexus cray-nexus` indicates the status is
    **NOT** `deployed`, then something is most likely wrong with the Nexus
    deployment and additional diagnosis is required. In this case, the current
    Nexus deployment probably needs to be uninstalled and the `nexus-data` PVC
    removed before attempting to deploy again.

